\documentclass{article}

% Language setting
% Replace `english' with e.g. `spanish' to change the document language
\usepackage[english]{babel}

% Set page size and margins
% Replace `letterpaper' with `a4paper' for UK/EU standard size
\usepackage[letterpaper,top=2cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[table]{xcolor}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\usepackage[
backend=biber,
style=ieee,
]{biblatex}


\addbibresource{sample.bib} %Imports bibliography file

\begin{document}

% ---------------------------
% Title page (custom)
% ---------------------------
\begin{titlepage}
  \begin{center}
    \includegraphics[width=5cm]{nup_logo.png} \\[1cm]
    {\Large Neapolis University Pafos} \\[0.5cm]

    {\large \textbf{Course Code:} IS509} \\[2cm]

    {\huge \textbf{Task 4: Research Questions and Hypotheses}} \\[1.5cm]

    {\large \textbf{Name:} Aleksandr Petrunin} \\[0.3cm]
    {\large \textbf{Student ID:} 1251114137} \\[2cm]

    {\large \today}
  \end{center}
\end{titlepage}

\section{Introduction (Task 3 recap)}
In Task 3, we explored the Agent-to-Agent (A2A) communication framework, which facilitates interaction among autonomous agents in distributed systems. 
A2A's extension mechanism allows agents to dynamically adapt their communication protocols based on context and requirements. 
However, while A2A provides a flexible structure for agent interactions, it lacks a formal semantic layer that ensures deterministic communication and verifiable coordination among agents. 
This gap presents challenges in scenarios where agents must reliably delegate tasks, negotiate roles, or coordinate actions without ambiguity.

This gap raises critical questions: How can agents achieve deterministic communication within A2A's extension mechanism? What formal semantics are necessary for verifiable multi-agent coordination? Can a typed semantic layer maintain A2A's flexibility while enabling reliable task delegation and negotiation?


\section{Research Questions and Hypotheses}

\paragraph{Refined questions}
\textbf{Goal:} Clarity + Feasibility + Relevance.\\
\textbf{Strategy:} 
\subparagraph{Attempt 1}
\begin{enumerate}
  \item How can agents achieve deterministic communication within A2A's extension mechanism?
  \item What formal semantics are required  for verifiable multi-agent coordination?
  \item Can a typed semantic layer be integrated into A2A without compromising its adaptability?
\end{enumerate}

\subparagraph{Attempt 2}
\begin{enumerate}
  \item How can LLM-based agents achieve \textcolor{red}{deterministic} communication within A2A's extension mechanism?
  \item Will the integration of a typed semantic layer improve task delegation and negotiation among LLM-based agents?
  \item What \textcolor{red}{formal semantics} are required  for \textcolor{red}{verifiable and deterministic coordination} of the LLM-based agents?
  \item Can a typed semantic layer be integrated into A2A framework without \textcolor{red}{compromising its adaptability}?
  \item Will the proposed semantic layer affect small and large scale MAS \textcolor{red}{differently}?
  \item Will the proposed semantic layer affect MAS made of SLMs, LLMs, hybrid set \textcolor{red}{differently}?
\end{enumerate}

\subparagraph{Attempt 3-5}
\begin{enumerate}
  \item [Does it work?](Casual) To what extent does integration of a typed semantic layer in A2A bring determinism (measured by task completion consistency) in collaborative activities among LLM-based agents compared to standard A2A?
  \item [Why does it work?](Descriptive) What formal semantic constructs (e.g., type systems, truth conditions, predicates) are required to achieve deterministic task delegation and role negotiation in LLM-based multi-agent systems?
  \item [When does it work?](Comparative/relational) How does coordination overhead and task completion accuracy of the typed semantic layer scale with MAS size (2, 5, and 10 agents) in the A2A framework?
  \item [For whom does it work?](Comparative/relational) Does the effectiveness of the typed semantic layer differ between MAS using small language models ($<$7B parameters) versus foundation models ($>$70B parameters) in terms of protocol adherence and negotiation success rates?
\end{enumerate}


\subsection{Topic1: Does it work?}
\paragraph{Research Question:} To what extent does integration of a typed semantic layer in A2A bring determinism (measured by task completion consistency) in collaborative activities among LLM-based agents compared to standard A2A?
\paragraph{Hypothesis:} The integration of a typed semantic layer in A2A will enhance determinism in collaborative activities among LLM-based agents, leading to higher task completion consistency (at least 15\% increase) compared to standard A2A without a semantic layer.
\paragraph{IV:} Integration of a typed semantic layer in A2A (with vs. without).
\paragraph{DV:} Task completion consistency (measured by the percentage of successful task completions across multiple trials).
\paragraph{Explanation:} If we get a 15\% or more increase in task completions on some random MAS setup, it indicates that the typed semantics might actually help agents interpret and execute tasks more reliably. 
This would provide evidence to support the alternative hypothesis that a formal semantic layer enhances determinism in multi-agent collaborations.
This evidence would allow to further explore the specific semantic constructs that contribute to this improvement, guiding future enhancements to the A2A framework.
\textbf{Alternatively}, if no improvement is observed, it would suggest that other factors may be more critical in achieving deterministic communication among LLM-based agents. In this case we rethink out approach.

\newpage
\subsection{Topic2: Why does it work?}
\paragraph{Research Question:} What formal semantic constructs (e.g., type systems, truth conditions, predicates) are required to achieve deterministic task delegation and role negotiation in LLM-based multi-agent systems?
\paragraph{Hypothesis:} Deterministic task delegation and role negotiation in LLM-based multi-agent systems emerge when agents share a typed interaction protocol—comprising (1) a shared ontology of task types, (2) well-formed truth-conditional commitments, and (3) role-dependent inference rules—allowing messages to be parsed into unambiguous semantic actions.
\paragraph{IV:} The degree of semantic formalization in the agent communication protocol.
\begin{enumerate}
  \item Presence/absence of type annotations on messages
  \item Presence/absence of role predicates
  \item Presence/absence of truth-conditional constraints
  \item Level of ontology structure (none → flat → hierarchical)
\end{enumerate}
\paragraph{DV:} Determinism of delegation and negotiation outcomes. Measured by:
\begin{enumerate}
  \item Variance in task-assignment decisions across runs
  \item Success rate of resolving role conflicts
  \item Number of negotiation cycles required
  \item Agreement consistency across agents
\end{enumerate}
\paragraph{Explanation:} The study investigates whether LLM agents achieve more predictable coordination when their communication is constrained by explicit semantic formalisms. Without such constraints, agents rely on natural-language inference, which introduces ambiguity and stochastic interpretation. By adding elements such as type systems, role predicates, and truth-conditional commitments, agent messages become machine-interpretable semantic acts rather than free-form text. If determinism increases with stronger semantic structure, it would support the hypothesis that LLM-multi-agent systems require lightweight formal semantics to achieve reliable delegation and negotiation.

\newpage
\subsection{Topic3: When does it work?}
\paragraph{Research Question:} How does coordination overhead and task completion accuracy of the typed semantic layer scale with MAS size (2, 5, and 10 agents) in the A2A framework?
\paragraph{Hypothesis:} As the number of agents in the MAS increases, the coordination overhead introduced by the typed semantic layer will grow sub-linearly, while task completion accuracy will improve up to a certain threshold (e.g., 5 agents), after which it will plateau or slightly decline due to increased communication complexity.
\paragraph{IV:} Number of agents in the MAS (2, 5, and 10).
\paragraph{DV:} Coordination overhead and task completion accuracy.
\paragraph{Explanation:} By varying the number of agents in the MAS, we can observe how the typed semantic layer affects efficiency and effectiveness of agent coordination. 
Coordination overhead can be measured by metrics such as message volume, latency, and processing time. 
Task completion accuracy can be assessed by the percentage of successfully completed tasks. 
If the hypothesis holds true, it would suggest that the typed semantic layer is effective in managing communication complexity up to a certain point, beyond which additional agents may introduce diminishing returns due to increased interaction complexity. 
This insight would inform the scalability limits of the proposed semantic layer within the A2A framework.

\newpage
\subsection{Topic4: For whom does it work?}
\paragraph{Research Question:} Does the effectiveness of the typed semantic layer differ between MAS using small language models ($<$7B parameters) versus foundation models ($>$70B parameters) in terms of protocol adherence and negotiation success rates?
\paragraph{Hypothesis:} The typed semantic layer will yield greater improvements in protocol adherence and negotiation success rates for MAS using small language models compared to those using foundation models, due to the limited inherent language understanding capabilities of smaller models.
\paragraph{IV:} Model size category (small language models $<$7B parameters, foundation models $>$70B parameters).
\paragraph{DV:} Protocol adherence and negotiation success rates.
\paragraph{Explanation:} By comparing MAS composed of small language models versus foundation models, we can assess how model capabilities influence the effectiveness of the typed semantic layer.
Small models may struggle with natural language nuances, making them more reliant on structured semantics for accurate communication. 
In contrast, foundation models possess advanced language understanding, potentially reducing their dependence on formal semantics.
If the hypothesis is confirmed, it would indicate that the typed semantic layer is particularly beneficial for enhancing communication reliability in MAS with limited language models, guiding future design considerations for multi-agent systems based on model capabilities.


\newpage
\subsection{Methodology}
To test these hypotheses, we will implement the typed semantic layer within the A2A framework and conduct a series of experiments varying the independent variables as outlined above.
We will measure the dependent variables using quantitative metrics such as task completion consistency, negotiation success rates, and coordination overhead.
Statistical analysis will be performed to evaluate the significance of observed effects and validate the hypotheses.


\subsubsection{Experimental Design}
The study will employ a mixed-methods approach combining quantitative experiments and qualitative analysis:
\begin{itemize}
  \item \textbf{Factorial design} to test interactions between semantic formalization levels and agent configurations
  \item \textbf{Repeated measures} (minimum 30 trials per condition) to account for LLM stochasticity
  \item \textbf{Baseline comparison} against standard A2A without semantic layer
  \item \textbf{Control variables}: task complexity, domain knowledge, message frequency
\end{itemize}

\subsubsection{Implementation}
\begin{itemize}
  \item Development of typed semantic layer prototype extending A2A's extension mechanism
  \item Integration with existing LLM inference frameworks (e.g., LangChain, AutoGen)
  \item Creation of benchmark task suites covering delegation, negotiation, and coordination scenarios
  \item Implementation of logging and monitoring infrastructure for all agent interactions
\end{itemize}

\subsubsection{Data Collection}
Quantitative metrics will include:
\begin{itemize}
  \item Task completion consistency (success rate across trials)
  \item Protocol adherence score (percentage of messages following typed schema)
  \item Negotiation success rates and cycle counts
  \item Message volume, latency, and processing time
  \item Agreement consistency (inter-agent alignment on decisions)
\end{itemize}

Qualitative data will capture:
\begin{itemize}
  \item Agent conversation logs for failure analysis
  \item Semantic ambiguity instances
  \item Edge cases where formal semantics break down
\end{itemize}

\subsubsection{Statistical Analysis}
\begin{itemize}
  \item ANOVA or Kruskal-Wallis tests for comparing conditions
  \item Post-hoc pairwise comparisons with Bonferroni correction
  \item Effect size calculations (Cohen's d) to assess practical significance
  \item Regression analysis for scaling behavior (Topic 3)
  \item Significance threshold: $\alpha = 0.05$
\end{itemize}

\subsubsection{Validation}
\begin{itemize}
  \item Cross-validation across different task domains
  \item Ablation studies to isolate contributions of individual semantic constructs
  \item Comparison with existing multi-agent coordination frameworks
  \item Reproducibility measures: fixed random seeds, version-controlled code, documented hyperparameters
\end{itemize}

\subsubsection{Additional considerations}
\begin{itemize}
  \item Participant models: Specify which LLM models we will use (e.g., GPT-4, Claude, Llama variants)
  \item Ethical considerations: Address potential biases in LLM behavior
  \item Timeline: Estimated duration for each experimental phase
  \item Limitations: Acknowledge constraints (computational resources, generalizability)
  \item Tools and infrastructure: Specific frameworks, hardware requirements, cloud services
\end{itemize}



\newpage
\printbibliography
\nocite{*}

\end{document}
